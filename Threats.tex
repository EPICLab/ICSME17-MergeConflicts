\section{Threats to Validity}\label{threats}
\subsubsection{\underline{Construct}}
Interview questions were open-ended and designed to elicit practitioner opinions about the experiences, difficulties, and perceptions of merge conflicts.
The particular factors and needs were determined after concluding all interviews, and thus did not bias interview participants to only factors previously mentioned.
Survey questions were created using factors found through card-based unitization.
This methodology allowed us to capture the common themes that practitioners experience when working with merge conflicts, but might have allowed themes specific to particular sub-groups to be unrepresented in our results.
\subsubsection{\underline{Internal Validity}}
Central tendency bias~\cite{guilford1954psychometric} is an issue in studies with 5-point Likert scales, since participants tend to choose less opinionated answers.
We lessen this effect by examining the answers in comparison to each other, as opposed to analysis of absolute mean values.
Because we use this method to highlight stronger answers by degree, this also means that we may have missed subtle trends across our data that may have been visible otherwise.

\subsubsection{\underline{External Validity}}
Interview results may not be generalizable to all practitioners due to a small sample size.
In order to reduce this effect, interview participants were selected from open- and closed-source projects, varying industries, and varying project sizes (see Table \ref{interview_demographics}).
To expand and confirm our interview results, we used a survey of 162 practitioners to ensure our results match with trends in the larger software development community.
We also recognize that recruitment of participants through digital communication methods is subject to self-selection bias and non-response bias.

\subsubsection{\underline{Reliability}}
The possibility exists that practitioner opinion will change over time or with a different sample group of survey participants.
By surveying 162 practitioners from varying roles, experience levels, and team sizes, we take a representative sample of the larger population of software practitioners.
Population sampling, including surveys, are traditionally validated for reliability through disclosure of response rates.
However, we used social media and mailing lists which do not allow accurate measurements of the number of individuals that read our recruitment requests.
Therefore we do not report a survey response rate.

The interview script and survey questions can be found on our website, see~\cite{companion_site}.

